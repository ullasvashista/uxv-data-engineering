{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "* Project: Denodo Query Logs\n",
    "* Author: Ullas Vashista\n",
    "* Last Update: 11/09/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import re\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CONFIG -------------------\n",
    "# Create parser and read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"./config/config.ini\")\n",
    " \n",
    "# Read values\n",
    "# Storage - Source\n",
    "source_storage_account_name = config.get(\"SourceStorage\", \"account_name\")\n",
    "source_container_name       = config.get(\"SourceStorage\", \"container_name\")\n",
    "source_mount_name           = config.get(\"SourceStorage\", \"mount_name\")\n",
    " \n",
    "# Storage - Target\n",
    "logm_storage_account_name = config.get(\"TargetStorage\", \"account_name\")\n",
    "logm_container_name       = config.get(\"TargetStorage\", \"container_name\")\n",
    "logm_mount_name           = config.get(\"TargetStorage\", \"mount_name\")\n",
    "\n",
    "# Key vault scope name\n",
    "scope_name = config.get(\"KeyVaultScope\", \"scope_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets\n",
    "client_id     = dbutils.secrets.get(scope=scope_name, key=\"adls-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=scope_name, key=\"adls-client-secret\")\n",
    "tenant_id     = dbutils.secrets.get(scope=scope_name, key=\"tenant-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "source_path = f\"abfss://{source_container_name}@{source_storage_account_name}.dfs.core.windows.net/{source_mount_name}\"\n",
    "delta_table_path = f\"abfss://{logm_container_name}@{logm_storage_account_name}.dfs.core.windows.net/{logm_mount_name}/delta/denodo_query_logs\"\n",
    "checkpoint_table = f\"abfss://{logm_container_name}@{logm_storage_account_name}.dfs.core.windows.net/{logm_mount_name}/delta/logm_denodo_query_checkpoint\"\n",
    " \n",
    "# File structures\n",
    "directories = [\"AVDP1\", \"AVDP2\"]\n",
    "file_prefixes = {\n",
    "    \"AVDP1\": \"dev_azure_vdp1-queries.log.\",\n",
    "    \"AVDP2\": \"dev_azure_vdp2-queries.log.\"\n",
    "}\n",
    " \n",
    "# Optional manual override\n",
    "start_date_str = \"09102025\"  # Only used if no checkpoint exists\n",
    "default_start_date = datetime.strptime(start_date_str, \"%m%d%Y\")\n",
    "end_date = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FUNCTIONS -------------------\n",
    " \n",
    "# Spark Configuration for ADLS Mounting\n",
    "def configure_spark_for_adls_oauth(storage_account_name, client_id, client_secret, tenant_id):\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\",\n",
    "                   \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\",\n",
    "                   f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "#\n",
    "def extract_date_from_filename(filename):\n",
    "    match = re.search(r\"\\.(\\d{8})$\", filename)\n",
    "    return datetime.strptime(match.group(1), \"%m%d%Y\").date() if match else None\n",
    " \n",
    "#To Get File Paths to read from\n",
    "def list_latest_file_paths(last_file_date, start_date, end_date, source_path, directories, file_prefixes):\n",
    "    file_paths = []\n",
    " \n",
    "    # Determine starting date\n",
    "    current_date1 = start_date if last_file_date is None else last_file_date\n",
    "    current_date = datetime.strptime(current_date1, \"%m%d%Y\")\n",
    " \n",
    "    # Generate file paths from current_date to end_date\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%m%d%Y\")\n",
    "        for dir_name in directories:\n",
    "            file_name = file_prefixes.get(dir_name, \"\") + date_str\n",
    "            full_path = f\"{source_path}/{dir_name}/{file_name}\"\n",
    "            file_paths.append(full_path)\n",
    "        current_date += timedelta(days=1)\n",
    " \n",
    "    return file_paths\n",
    " \n",
    "# Look up for checkpoints\n",
    "def get_checkpoint():\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(checkpoint_table)\n",
    "        df = df.withColumn(\"reversed_parsed_date\",to_date(col(\"file_date\"), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"date_MMddyyyy\",date_format(col(\"reversed_parsed_date\"), \"MMddyyyy\"))\n",
    "        row = df.orderBy(col(\"file_date\").desc()).first()\n",
    "       \n",
    "        return row[\"date_MMddyyyy\"], row[\"last_timegenerated\"]\n",
    "    except AnalysisException:\n",
    "        return None, None\n",
    " \n",
    "#Load latest Metadata to Checkpoints\n",
    "def update_checkpoint(file_date, last_timegenerated):\n",
    "    df = spark.createDataFrame([(file_date, last_timegenerated)], [\"file_date\", \"last_timegenerated\"])\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(checkpoint_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "# ------------------- MAIN -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source ADLS config\n",
    "configure_spark_for_adls_oauth(source_storage_account_name, client_id, client_secret, tenant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check last processed file and TimeGenerated\n",
    "last_file_date, last_timegenerated = get_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get all available log files and latest file\n",
    "paths_to_read = list_latest_file_paths(last_file_date, start_date_str, end_date, source_path, directories, file_prefixes)\n",
    "if not paths_to_read:\n",
    "    raise Exception(\"No log files found in source.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Read raw data\n",
    "raw_df = spark.read.option(\"mode\", \"PERMISSIVE\").json(paths_to_read)\n",
    " \n",
    "if raw_df.rdd.isEmpty():\n",
    "    print(\"No data found.\")\n",
    "    dbutils.notebook.exit(\"EMPTY\")\n",
    " \n",
    "# Add source_dir\n",
    "raw_df = raw_df.withColumn(\"source_dir\", regexp_extract(input_file_name(), r\"/(AVDP[123])/\", 1))\n",
    "raw_df = raw_df.withColumn(\"TimeGenerated\", col(\"@timestamp\").cast(TimestampType()))\n",
    "raw_df = raw_df.withColumn(\"input_file_path\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter based on last processed TimeGenerated\n",
    "if last_timegenerated:\n",
    "    raw_df = raw_df.filter(col(\"TimeGenerated\") > last_timegenerated)\n",
    " \n",
    "if raw_df.rdd.isEmpty():\n",
    "    print(\"No new TimeGenerated records.\")\n",
    "    dbutils.notebook.exit(\"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split log and extract columns\n",
    "split_df = raw_df.withColumn(\"log_split\", split(\"log\", \"\\t\"))\n",
    "final_df = split_df.select(\n",
    "    col(\"TimeGenerated\"),\n",
    "    col(\"log_split\")[0].alias(\"ServerName\"),\n",
    "    col(\"log_split\")[1].alias(\"Host\"),\n",
    "    col(\"log_split\")[2].alias(\"Port\"),\n",
    "    col(\"log_split\")[3].alias(\"Id_\"),\n",
    "    col(\"log_split\")[4].alias(\"UserName\"),\n",
    "    col(\"log_split\")[5].alias(\"DatabaseName\"),\n",
    "    col(\"log_split\")[6].alias(\"NotificationType\"),\n",
    "    col(\"log_split\")[7].alias(\"SessionId\"),\n",
    "    col(\"log_split\")[8].alias(\"StartTime\"),\n",
    "    col(\"log_split\")[9].alias(\"EndTime\"),\n",
    "    col(\"log_split\")[10].alias(\"Duration\"),\n",
    "    col(\"log_split\")[11].alias(\"WaitingTime\"),\n",
    "    col(\"log_split\")[12].alias(\"NumRows\"),\n",
    "    col(\"log_split\")[13].alias(\"State\"),\n",
    "    col(\"log_split\")[14].alias(\"Completed\"),\n",
    "    col(\"log_split\")[15].alias(\"Cache\"),\n",
    "    col(\"log_split\")[16].alias(\"Query\"),\n",
    "    col(\"log_split\")[17].alias(\"RequestType\"),\n",
    "    col(\"log_split\")[18].alias(\"Element\"),\n",
    "    col(\"log_split\")[19].alias(\"UserAgent\"),\n",
    "    col(\"log_split\")[20].alias(\"AccessInterface\"),\n",
    "    col(\"log_split\")[21].alias(\"ClientIP\"),\n",
    "    col(\"log_split\")[22].alias(\"TransactionId\"),\n",
    "    col(\"log_split\")[23].alias(\"WebServiceName\"),\n",
    "    col(\"input_file_path\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add Metadata Columns\n",
    "final_df = (\n",
    "    final_df\n",
    "    .withColumn(\"body\", to_json(struct(*final_df.columns)))\n",
    "    .withColumn(\"MetadataLogId\", sha2(col(\"body\"), 512).cast(\"string\"))\n",
    "    .drop(\"body\")\n",
    "    .withColumn(\"MetadataLogType\", lit(\"logm.denodo.queries\"))\n",
    "    .withColumn(\"MetadataLogTimeGenerated\", col(\"TimeGenerated\").cast(TimestampType()))\n",
    "    .withColumn(\"MetadataLogDate\", date_format(col(\"TimeGenerated\"), \"yyyyMMddHHmmssSSS\").cast(LongType()))\n",
    "    .drop(\"TimeGenerated\")\n",
    "    .withColumn(\"TimeGenerated\", current_timestamp().cast(StringType()))\n",
    "    .withColumn(\"MetadataLogWindow\", date_format(from_utc_timestamp(col(\"MetadataLogTimeGenerated\"), \"UTC\"), \"yyMMddHHmm\").cast(LongType()))\n",
    "    .withColumn(\"MetadataLogGuid\", concat(col(\"MetadataLogWindow\"), lpad(monotonically_increasing_id(), 9, \"0\")).cast(LongType()))\n",
    "    .drop(\"MetadataLogWindow\")\n",
    "    .withColumn(\"MetadataLogFileName\", col(\"input_file_path\"))\n",
    "    .drop(\"input_file_path\")\n",
    "    .withColumn(\"TenantId\", lit(tenant_id))\n",
    "    .withColumn(\"Type\", lit(\"LOGM_DENODO_QUERIES_CL\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Write to Delta\n",
    "configure_spark_for_adls_oauth(logm_storage_account_name, client_id, client_secret, tenant_id)\n",
    "final_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Update checkpoint\n",
    "last_timegenerated = final_df.agg({\"MetadataLogTimeGenerated\": \"max\"}).collect()[0][0]\n",
    " \n",
    "input_file_date = final_df.withColumn(\"input_file_date\",regexp_extract(col(\"MetadataLogFileName\"), r\"\\.(\\d{8})$\", 1))\n",
    "input_file_date = input_file_date.withColumn(\"parsed_date\",to_date(col(\"input_file_date\"), \"MMddyyyy\"))\n",
    "input_file_date = input_file_date.withColumn(\"date_yyyymmdd\",date_format(col(\"parsed_date\"), \"yyyyMMdd\"))\n",
    "file_date = input_file_date.agg({\"date_yyyymmdd\": \"max\"}).collect()[0][0]\n",
    " \n",
    "update_checkpoint(file_date, last_timegenerated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
